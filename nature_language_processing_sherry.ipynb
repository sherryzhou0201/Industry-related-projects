{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " nature language processing_sherry.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherryzhou0201/Industry-related-projects/blob/master/nature_language_processing_sherry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3LTq59E8UK"
      },
      "source": [
        "# Document Clustering and Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS-ob0Q8E8UM"
      },
      "source": [
        "*Data source* : It is a e-commerce product review data\n",
        "\n",
        " In this project, we used unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE7Qr-ccE8UN"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S0waFW9E8UO"
      },
      "source": [
        "<ul>\n",
        "<li>[Part 1: Load Data]\n",
        "<li>[Part 2: Tokenizing and Stemming]\n",
        "<li>[Part 3: Feature Enginerring: TF-IDF]\n",
        "<li>[Part 4: Topic Modeling - Latent Dirichlet Allocation]\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nXGAelMjJFq"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eT1n7oijJ8v"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N2bb9S6jKGh"
      },
      "source": [
        "file = drive.CreateFile({'id':'1LOjyleXj_vESjl3c9miOw06gXWW31c_W'}) # replace the id with id of file you want to access\n",
        "file.GetContentFile('watch_reviews.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc9DK62BE8UP"
      },
      "source": [
        "# Part 1: Load Data\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdBV8gGE8UQ",
        "outputId": "86277328-f00b-4ed3-8298-6059358be5be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# REGULAR EXPRESSION\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')  \n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJBDZNCbTh-g"
      },
      "source": [
        "Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVsLtiVfE8UU"
      },
      "source": [
        "Read data from files. In summary, we have 960204 observations, and 15 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKzTgR0DYmht",
        "outputId": "e5cf61c1-5c23-488c-d4db-14fb7fb2dbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# load data into dataframe\n",
        "df=pd.read_csv('watch_reviews.tsv',sep='\\t',header=0, error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 8704: expected 15 fields, saw 22\\nSkipping line 16933: expected 15 fields, saw 22\\nSkipping line 23726: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 85637: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 132136: expected 15 fields, saw 22\\nSkipping line 158070: expected 15 fields, saw 22\\nSkipping line 166007: expected 15 fields, saw 22\\nSkipping line 171877: expected 15 fields, saw 22\\nSkipping line 177756: expected 15 fields, saw 22\\nSkipping line 181773: expected 15 fields, saw 22\\nSkipping line 191085: expected 15 fields, saw 22\\nSkipping line 196273: expected 15 fields, saw 22\\nSkipping line 196331: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 197000: expected 15 fields, saw 22\\nSkipping line 197011: expected 15 fields, saw 22\\nSkipping line 197432: expected 15 fields, saw 22\\nSkipping line 208016: expected 15 fields, saw 22\\nSkipping line 214110: expected 15 fields, saw 22\\nSkipping line 244328: expected 15 fields, saw 22\\nSkipping line 248519: expected 15 fields, saw 22\\nSkipping line 254936: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 272057: expected 15 fields, saw 22\\nSkipping line 293214: expected 15 fields, saw 22\\nSkipping line 310507: expected 15 fields, saw 22\\nSkipping line 312306: expected 15 fields, saw 22\\nSkipping line 316296: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 336028: expected 15 fields, saw 22\\nSkipping line 344885: expected 15 fields, saw 22\\nSkipping line 352551: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 408773: expected 15 fields, saw 22\\nSkipping line 434535: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 581593: expected 15 fields, saw 22\\n'\n",
            "b'Skipping line 652409: expected 15 fields, saw 22\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdNKK0s6ZtdR",
        "outputId": "17b75efb-c347-48ae-ddb1-f4c936e473ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>3653882</td>\n",
              "      <td>R3O9SGZBVQBV76</td>\n",
              "      <td>B00FALQ1ZC</td>\n",
              "      <td>937001370</td>\n",
              "      <td>Invicta Women's 15150 \"Angel\" 18k Yellow Gold ...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Absolutely love this watch! Get compliments al...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>14661224</td>\n",
              "      <td>RKH8BNC3L5DLF</td>\n",
              "      <td>B00D3RGO20</td>\n",
              "      <td>484010722</td>\n",
              "      <td>Kenneth Cole New York Women's KC4944 Automatic...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>I love thiswatch it keeps time wonderfully</td>\n",
              "      <td>I love this watch it keeps time wonderfully.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>27324930</td>\n",
              "      <td>R2HLE8WKZSU3NL</td>\n",
              "      <td>B00DKYC7TK</td>\n",
              "      <td>361166390</td>\n",
              "      <td>Ritche 22mm Black Stainless Steel Bracelet Wat...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Two Stars</td>\n",
              "      <td>Scratches</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>7211452</td>\n",
              "      <td>R31U3UH5AZ42LL</td>\n",
              "      <td>B000EQS1JW</td>\n",
              "      <td>958035625</td>\n",
              "      <td>Citizen Men's BM8180-03E Eco-Drive Stainless S...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>It works well on me. However, I found cheaper ...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>12733322</td>\n",
              "      <td>R2SV659OUJ945Y</td>\n",
              "      <td>B00A6GFD7S</td>\n",
              "      <td>765328221</td>\n",
              "      <td>Orient ER27009B Men's Symphony Automatic Stain...</td>\n",
              "      <td>Watches</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Beautiful face, but cheap sounding links</td>\n",
              "      <td>Beautiful watch face.  The band looks nice all...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  marketplace  ...  review_date\n",
              "0          US  ...   2015-08-31\n",
              "1          US  ...   2015-08-31\n",
              "2          US  ...   2015-08-31\n",
              "3          US  ...   2015-08-31\n",
              "4          US  ...   2015-08-31\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ag01EYLaMO8",
        "outputId": "f992eb7b-33fd-401c-e98c-21762a548176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# number of records \n",
        "print(len(df)) # 960204\n",
        "df.shape\n",
        "df[\"customer_id\"].nunique() # number of unique customers -- 719522"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "960204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "719522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HnleJXraXNp"
      },
      "source": [
        "# remove missing values\n",
        "df.review_body.dropna(inplace=True) # return a new seris with missing value removed "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSSfq59qa8GS",
        "outputId": "c475414d-f6be-4d71-9a9d-3552e2ecab71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "960204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_3rRoVRcc8H"
      },
      "source": [
        "# use the first 1000 data as the trianing data \n",
        "data=df.loc[:1000,'review_body'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4KGnVeE8UX"
      },
      "source": [
        "# Part 2: Tokenizing and Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "GHHIaFATE8UY"
      },
      "source": [
        "Load stopwords and stemmer function from NLTK library.\n",
        "Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning.\n",
        "Stemming is the process of breaking a word down into its root.\n",
        "\n",
        "NLTK library:\n",
        "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gSwiUBRE8UY",
        "outputId": "537d404c-b9e5-4953-c32a-bec68ea48c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Use nltk's English stopwords.\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n",
        "print (stopwords[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We use 179 stop-words from nltk library.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e50130X8E8Uc"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# tokenization and stemming\n",
        "def tokenization_and_stemming(text):\n",
        "    # exclude stop words and tokenize the document, generate a list of string \n",
        "    # tokenize sentence first, and then tokenize the word for each stentence, and then for the words in the sentence, pick the one not in the stopwrods.\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
        "\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    # re. -reg expression\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "            \n",
        "    # stemming\n",
        "    # stemmer only deal with suffix, but not the prefix\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "# tokenization without stemming\n",
        "def tokenization(text):\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word not in stopwords]\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PBMVVMNRX0q",
        "outputId": "8fc04075-59de-4459-8b46-086b413215e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check what the tokens look like \n",
        "tokens=[word.lower() for sent in nltk.sent_tokenize(data[0]) for word in nltk.word_tokenize(sent)]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['absolutely', 'love', 'this', 'watch', '!', 'get', 'compliments', 'almost', 'every', 'time', 'i', 'wear', 'it', '.', 'dainty', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lGa6HEHgkAx",
        "outputId": "c09263f6-55ae-43c8-caed-38aa85558976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# check the whole function.\n",
        "# tokenization and stemming in the real data set\n",
        "tokenization_and_stemming(data[0]) # the first row"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['absolut',\n",
              " 'love',\n",
              " 'watch',\n",
              " 'get',\n",
              " 'compliment',\n",
              " 'almost',\n",
              " 'everi',\n",
              " 'time',\n",
              " 'i',\n",
              " 'wear',\n",
              " 'dainti']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtDXMCeME8Uh"
      },
      "source": [
        "Use our defined functions to analyze (i.e. tokenize, stem) our synoposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNtXZ3RlE8Ui"
      },
      "source": [
        "# 1. do tokenization and stemming for all the documents\n",
        "# 2. also just do tokenization for all the documents\n",
        "# the goal is to create a mapping from stemmed words to original tokenized words for result interpretation.\n",
        "docs_stemmed = []\n",
        "docs_tokenized = []\n",
        "\n",
        "for i in data:\n",
        "    tokenized_and_stemmed_results = tokenization_and_stemming(i)\n",
        "    docs_stemmed.extend(tokenized_and_stemmed_results) # create the vocabulary with the stem\n",
        "    \n",
        "    tokenized_results = tokenization(i)\n",
        "    docs_tokenized.extend(tokenized_results) # create the vocabulary without the stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj7JZxnpE8Uk"
      },
      "source": [
        "Create a mapping from stemmed words to original tokenized words for result interpretation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hM6bG3xUhSL",
        "outputId": "f3ed65cd-934a-43be-e53a-31196359c5dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(docs_stemmed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18080"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoTp63fME8Ul",
        "outputId": "17cd9ae5-4353-477c-febe-e909edd3b5c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# craete a dictionary with mapped words with /without suffix\n",
        "vocab_frame_dict = {docs_stemmed[x]:docs_tokenized[x] for x in range(len(docs_stemmed))}\n",
        "print (vocab_frame_dict['angel'])\n",
        "# this is an exmaple that angeles, when get rid of the suffix, it is shown as angel."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "angeles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "QAWdFqL5E8Uo"
      },
      "source": [
        "# Part 3: TF-IDF\n",
        "\n",
        "TF: Term Frequency\n",
        "\n",
        "IDF: Inverse Document Frequency\n",
        "\n",
        "***example:***\n",
        "\n",
        "document1: \"Arthur da Jason\"\n",
        "\n",
        "document 2: \"Jason da da huang\"\n",
        "\n",
        "document1: tf-idf [2, 0.5, 0.5, 0];  document2: tf-idf [0, 1, 0.5, 1]  \n",
        "\n",
        "2-gram: \n",
        "\n",
        "document 1: Arthur da, da Jason; document 2: Jason da, da da, da huang bigram\n",
        "\n",
        "3-gram:\n",
        "\n",
        "document 1: Athur da Jason;  document 2: Jason da da, da da huang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-XH7R4pE8Up",
        "outputId": "4d2b53c6-88a8-40c5-fb2d-2298204a1364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# define vectorizer parameters\n",
        "# TfidfVectorizer will help us to create tf-idf matrix\n",
        "# TfidfVectorizer is from sklearn\n",
        "# max_df : maximum document frequency for the given word\n",
        "# min_df : minimum document frequency for the given word\n",
        "# max_features: maximum number of words\n",
        "# use_idf: if not true, we only calculate tf\n",
        "# stop_words : built-in stop words\n",
        "# tokenizer: how to tokenize the document\n",
        "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
        "# 1-3 means we put 1 gram，2gram，3gram togeher ，and we only pick the first 2000个\n",
        "tfidf_model = TfidfVectorizer(max_df=0.8, max_features=2000,\n",
        "                                 min_df=0, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,3))\n",
        "\n",
        "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n",
        "\n",
        "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
        "      \" synoposes and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "In total, there are 1000 synoposes and 2000 terms.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoRH6IDVE8Ur",
        "outputId": "40986235-f47d-42d9-9bc2-b4ff1d16c6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# check the parameters\n",
        "tfidf_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'analyzer': 'word',\n",
              " 'binary': False,\n",
              " 'decode_error': 'strict',\n",
              " 'dtype': numpy.float64,\n",
              " 'encoding': 'utf-8',\n",
              " 'input': 'content',\n",
              " 'lowercase': True,\n",
              " 'max_df': 0.8,\n",
              " 'max_features': 2000,\n",
              " 'min_df': 0,\n",
              " 'ngram_range': (1, 3),\n",
              " 'norm': 'l2',\n",
              " 'preprocessor': None,\n",
              " 'smooth_idf': True,\n",
              " 'stop_words': 'english',\n",
              " 'strip_accents': None,\n",
              " 'sublinear_tf': False,\n",
              " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              " 'tokenizer': <function __main__.tokenization_and_stemming>,\n",
              " 'use_idf': True,\n",
              " 'vocabulary': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whu1pCOiE8Uv"
      },
      "source": [
        "Save the terms identified by TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdKk6n-E8Uw"
      },
      "source": [
        "# words\n",
        "tf_selected_words = tfidf_model.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWvzNWQFB26"
      },
      "source": [
        "# print out words\n",
        "# tf_selected_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6JFh-1BE8Uy",
        "outputId": "aaf0eba6-bc18-4e8d-c209-af4889fa282d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# tf-idf matrix\n",
        "tfidf_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1000x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 14489 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYOZXL53E8VV"
      },
      "source": [
        "# Part 4: Topic Modeling - Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBPVbFNFE8VW"
      },
      "source": [
        "# Use LDA for clustering\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=5, learning_method = 'online')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5YYz__7E8VY"
      },
      "source": [
        "# LDA requires integer values, keep first 3 digits\n",
        "tfidf_matrix_lda = (tfidf_matrix * 100)\n",
        "tfidf_matrix_lda = tfidf_matrix_lda.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIwUNN0ZE8Vb"
      },
      "source": [
        "lda.fit(tfidf_matrix_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TukTmm7dE8Vd"
      },
      "source": [
        "# topics and words matrix\n",
        "topic_word = lda.components_\n",
        "print(topic_word.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hs012V-E8Vg"
      },
      "source": [
        "# the number of words chosen seem to be tricky, I have not figured out which way to choose the best word\n",
        "# however ,in our case ,>2 and <2 does not seem to provide us very different results\n",
        "n_top_words = 7\n",
        "topic_keywords_list = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    #Here we select top(n_top_words-1)\n",
        "    lda_topic_words = np.array(tf_selected_words)[np.argsort(topic_dist)][:-n_top_words:-1] \n",
        "    for j in range(len(lda_topic_words)):\n",
        "        lda_topic_words[j] = vocab_frame_dict[lda_topic_words[j]]\n",
        "    topic_keywords_list.append(lda_topic_words.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuSg4tbSE8Vh"
      },
      "source": [
        "# documents and topics matri\n",
        "doc_topic = lda.transform(tfidf_matrix_lda)\n",
        "print (doc_topic.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paiC0DU-E8Vj"
      },
      "source": [
        "# print out the clusters and topics and titles of the movies\n",
        "topic_doc_dict = {}\n",
        "print (\"<Document clustering result by LDA>\")\n",
        "for i in range(len(doc_topic)):\n",
        "    topicID = doc_topic[i].argmax()\n",
        "    if topicID not in topic_doc_dict:\n",
        "        topic_doc_dict[topicID] = [titles[i]]\n",
        "    else:\n",
        "        topic_doc_dict[topicID].append(titles[i])\n",
        "for i in topic_doc_dict:\n",
        "    print (\"Cluster \" + str(i) + \" words: \" + \", \".join(topic_keywords_list[i]))\n",
        "    print (\"Cluster \" + str(i) + \" titles (\" + str(len(topic_doc_dict[i])) + \" movies): \")\n",
        "    print (', '.join(topic_doc_dict[i]))\n",
        "    print ()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}